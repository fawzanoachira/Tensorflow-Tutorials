{"cells":[{"cell_type":"markdown","metadata":{"id":"1FoOUVU9X4fD"},"source":["#**Recurrent Neural Networks (RNN)**\n","\n","Recurrent Neural Networks (RNNs) are a type of artificial neural network designed for sequence data processing, making them particularly suitable for tasks such as natural language processing, time series analysis, and speech recognition. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a hidden state that captures information about previous inputs in the sequence.\n","<br><br>\n","###Architecture of RNNs:\n",">Input Layer: Accepts input features at each time step.\n","\n",">Hidden Layer: Maintains a hidden state that captures information from previous time steps. This hidden state is updated at each time step and serves as a memory of the past.\n","\n",">Output Layer: Produces the output based on the hidden state. The output can be used for making predictions or further processing.\n","\n","![RNN Architecture](https://cdn.ttgtmedia.com/rms/onlineimages/enterpriseai-recurrent_neural_network-f.png)\n","\n","The key idea behind RNNs is the ability to maintain and update a hidden state by considering information from the current input and the previous hidden state.\n","\n","###Unrolling the Network:\n","The concept of \"unrolling\" an RNN helps in visualizing how the network processes sequences over time. Instead of thinking of the network as a single entity, you can imagine it as a series of connected copies, each representing one time step. This visualization helps understand how the hidden state is updated at each step and how information flows through the network.\n","\n","###Hidden State Updates:\n","The hidden state update in an RNN is based on the current input and the previous hidden state.\n","![hidden state update](https://miro.medium.com/v2/resize:fit:720/format:webp/1*pvYl2BASKV3WLclrIQykBA.png)"]},{"cell_type":"markdown","metadata":{"id":"0C4pGdcel1Xm"},"source":["#**Attention Mechanisms:**\n","\n","\n","Attention mechanisms have become a fundamental component in deep learning models, particularly in natural language processing tasks, computer vision, and sequence-to-sequence problems. Attention mechanisms allow models to focus on specific parts of the input sequence when making predictions, providing the capability to selectively weigh different parts of the input.<br><br>\n","\n","Here's an overview of attention mechanisms and how to implement attention layers in Keras:\n","\n","###**Attention Mechanism Overview:**\n","**Input Sequences:**\n","\n","In many sequence-based tasks, the input is a sequence of data, such as words in a sentence or frames in a video.\n","\n","**Encoder-Decoder Architecture:**\n","\n","Attention mechanisms are commonly used in encoder-decoder architectures. The encoder processes the input sequence, and the decoder generates the output sequence.\n","\n","**Context Vector:**\n","\n","Attention introduces the concept of a \"context vector\" that captures relevant information from the input sequence for each step in the output sequence.\n","\n","**Attention Weights:**\n","\n","Attention weights are computed to determine how much focus each element in the input sequence should receive. These weights are learned during the training process.\n","\n","**Weighted Sum:**\n","\n","The context vector is formed by taking a weighted sum of the input sequence elements, where the weights are determined by the attention weights.\n"]},{"cell_type":"markdown","metadata":{"id":"x4wtg4dhmYy3"},"source":["#**Implementing Attention Layers in Keras:**\n","\n","Attention class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CT_qhrqWme58"},"outputs":[],"source":["keras.layers.Attention(\n","    use_scale=False, score_mode=\"dot\", dropout=0.0, seed=None, **kwargs\n",")"]},{"cell_type":"markdown","metadata":{"id":"4MYLmYF8nkK8"},"source":["Dot-product attention layer, a.k.a. Luong-style attention.\n","\n","Inputs are a list with 2 or 3 elements:\n","1. A query tensor of shape (batch_size, Tq, dim).\n","2. A value tensor of shape (batch_size, Tv, dim).\n","3. A optional key tensor of shape (batch_size, Tv, dim). If none supplied, value will be used as a key.\n","\n","The calculation follows the steps:\n","1. Calculate attention scores using query and key with shape (batch_size, Tq, Tv).\n","2. Use scores to calculate a softmax distribution with shape (batch_size, Tq, Tv).\n","3. Use the softmax distribution to create a linear combination of value with shape (batch_size, Tq, dim).\n","\n","Arguments\n","\n","* use_scale: If True, will create a scalar variable to scale the attention scores.\n","\n","* dropout: Float between 0 and 1. Fraction of the units to drop for the attention scores. Defaults to 0.0.\n","\n","* seed: A Python integer to use as random seed incase of dropout.\n","\n","* score_mode: Function to use to compute attention scores, one of {\"dot\", \"concat\"}. \"dot\" refers to the dot product between the query and key vectors. \"concat\" refers to the hyperbolic tangent of the concatenation of the query and key vectors.\n","\n","Call # Arguments inputs: List of the following tensors: - query: Query tensor of shape (batch_size, Tq, dim). - value: Value tensor of shape (batch_size, Tv, dim). - key: Optional key tensor of shape (batch_size, Tv, dim). If not given, will use value for both key and value, which is the most common case. mask: List of the following tensors: - query_mask: A boolean mask tensor of shape (batch_size, Tq). If given, the output will be zero at the positions where mask==False. - value_mask: A boolean mask tensor of shape (batch_size, Tv). If given, will apply the mask such that values at positions where mask==False do not contribute to the result. return_attention_scores: bool, it True, returns the attention scores (after masking and softmax) as an additional output argument. training: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout). use_causal_mask: Boolean. Set to True for decoder self-attention. Adds a mask such that position i cannot attend to positions j > i. This prevents the flow of information from the future towards the past. Defaults to False.\n","\n","Output: Attention outputs of shape (batch_size, Tq, dim). (Optional) Attention scores after masking and softmax with shape (batch_size, Tq, Tv)."]},{"cell_type":"markdown","metadata":{"id":"W2s8RMCCn92M"},"source":[]},{"cell_type":"markdown","metadata":{"id":"0_SpViXZodvv"},"source":["#**Deep RNN Architectures**\n","\n","Deep Recurrent Neural Networks (RNNs) come in various architectures to handle sequential data effectively. Two popular variations are Bidirectional RNNs and Encoder-Decoder networks, each serving specific purposes in sequence processing tasks.\n","\n","##**Bidirectional RNNs:**\n","1. Unidirectional RNNs:\n","\n","Standard RNNs process sequences in a forward direction, where each element in the sequence is processed in order.\n","\n","2. Bidirectional RNNs:\n","\n","Bidirectional RNNs process sequences in both forward and backward directions. This allows the model to capture information from past and future time steps for each element in the sequence.\n","\n","3. Implementation in Keras:\n","\n","Keras provides the Bidirectional wrapper for RNN layers. You can wrap an LSTM or GRU layer with Bidirectional to create a bidirectional variant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTJ1qRalo0nU"},"outputs":[],"source":["from tensorflow.keras.layers import Bidirectional, LSTM\n","\n","model = Sequential()\n","model.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(timesteps, features)))\n","# Add more layers as needed"]},{"cell_type":"markdown","metadata":{"id":"OlpYEreIpFAD"},"source":["4. Use Cases:\n","\n","Bidirectional RNNs are beneficial when the context from both past and future time steps is essential for understanding the current element in the sequence. They are commonly used in tasks like sentiment analysis, named entity recognition, and speech recognition.\n"]},{"cell_type":"markdown","metadata":{"id":"PXmtUOKYpRaZ"},"source":["#**Encoder-Decoder Networks:**\n","\n","1. Basic Structure:\n","\n","Encoder-Decoder architectures consist of two main components: an encoder that processes the input sequence and a decoder that generates the output sequence.\n","\n","2. Sequence-to-Sequence Tasks:\n","\n","Encoder-Decoder networks are widely used for sequence-to-sequence tasks, such as machine translation, text summarization, and image captioning.\n","\n","3. Implementation in Keras:\n","\n","Keras provides the GRU, LSTM, or other RNN layers for both the encoder and decoder components. The RepeatVector layer is often used to repeat the encoder's output sequence to match the length of the target sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRcHZICQpfv-"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, RepeatVector, Dense\n","\n","model = Sequential()\n","model.add(LSTM(units=64, input_shape=(timesteps, features)))\n","model.add(RepeatVector(output_sequence_length))\n","model.add(LSTM(units=64, return_sequences=True))\n","# Add more layers as needed\n","model.add(Dense(output_dim=vocab_size, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"F25A5NU_pjcX"},"source":["4. Use Cases:\n","\n","Encoder-Decoder architectures are effective for tasks where the input and output sequences have different lengths, and the model needs to capture the entire context of the input sequence before generating the output. Applications include language translation, summarization, and conversation generation.\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r7EECVnLtA4V"},"source":["#**Text summarization with pointer networks**\n","\n","Text summarization with pointer networks is an approach that involves using a mechanism to point to specific words in the input text as part of the summary. This allows the model to selectively \"copy\" words from the input text rather than generating entirely new words. Pointer networks are particularly useful in handling out-of-vocabulary words and capturing important information directly from the input.\n","\n","Here's a general outline of how you can implement text summarization with pointer networks using a sequence-to-sequence model in Keras:\n","\n","##**Pointer Network for Text Summarization:**\n","1. Data Preparation:\n","\n","Prepare your dataset with pairs of input and target sequences, where the target sequence is the summary. Also, create vocabulary mappings for both input and output.\n","\n","2. Model Architecture:\n","\n","Create a sequence-to-sequence model using an encoder-decoder architecture.\n","\n","3. Encoder:\n","\n","Use an RNN (such as LSTM or GRU) to encode the input sequence.\n","\n","4. Decoder:\n","\n","Use an attention mechanism in the decoder to focus on different parts of the input sequence during the generation of each output word.\n","\n","5. Pointer Network:\n","\n","Implement a pointer network layer that calculates probabilities for pointing to each word in the input sequence.\n","\n","6. Loss Function:\n","\n","Use a custom loss function that combines standard sequence-to-sequence loss (e.g., categorical cross-entropy) and a pointing mechanism loss. The pointing mechanism loss encourages the model to correctly point to words in the input sequence."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOE5hVtjcnpCAs/L/v0pp2I","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
